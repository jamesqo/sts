\subsection{Motivation}

In order to highlight the benefit of growth arrays, I will use an alternative notation for time complexity. The reason for this is, for certain operations, growth arrays are only better than dynamic arrays by a constant factor. For example, dynamic arrays might allocate roughly $2n$ memory for appending $n$ items, while growth arrays would allocate roughly $n$ memory. Even though growth arrays are clearly better in this regard, the big-O space complexity for both data structures would be the same, $O(n)$.

My goal is to be able to compare the coefficients of the highest-order terms in both expressions. For example, I would like to take the ratio $\frac{2n}{n}$, see that it is $2$, and conclude that dynamic arrays allocate roughly twice as much as growth arrays for large $n$. However, big-O notation does not support this.

\subsection{Definition}

To analyze time complexity, I will use the $\sim$ relation. It is defined as follows:

\begin{align*}
f \sim g \leftrightarrow \lim_{n \to \infty} \frac{g}{f} = 1
\end{align*}

This is read as "$f$ is asymptotic to $g$" or "$f$ and $g$ are asymptotic." {\HdrNote} $f$ and $g$ are used as shorthand to denote $f(n)$ and $g(n)$, respectively.

Notice that while $O(2n) = O(n)$, $2n \not\sim n$. Thus, $\sim$ makes it possible to distinguish between a function that uses $n$ space and one that uses $2n$ space. {\HdrNote} A consequence of this is that bases for logarithms cannot be omitted, like in big-O notation.

\subsection{Properties}
\label{subsec:AsymptoticProperties}

Here, I define various properties of the $\sim$ relation that will be used in my proofs. Proofs of these properties can be found in the appendix.

The following two theorems state that $\sim$ can "merge", or un-distribute, over arithmetic operations. This is a property shared with big-O.

\begin{theorem}
\label{thm:MergesOverArithmeticAdd}
	$P$ merges over addition and subtraction. That is, for functions $f$ and $g$,
	\begin{align*}
	(\hat{f} \sim f) \land (\hat{g} \sim g) \rightarrow \begin{cases}
		(\hat{f} + \hat{g}) \sim (f + g)\\
		(\hat{f} - \hat{g}) \sim (f - g)
	\end{cases}
	\end{align*}
\end{theorem}

\begin{theorem}
\label{thm:MergesOverArithmeticMultiply}
	$P$ merges over multiplication and division. That is, for functions $f$ and $g$,
	\begin{align*}
	(\hat{f} \sim f) \land (\hat{g} \sim g) \rightarrow \begin{cases}
	\hat{f}\hat{g} \sim fg\\
	\hat{f} / \hat{g} \sim f / g
	\end{cases}
	\end{align*}
\end{theorem}

The following theorem states that lower-order terms may be removed. For example, $(n + \log_2 n) \sim n$.

\begin{theorem}
\label{thm:RemovesLowerOrderTerms}
	If $\lim_{n \to \infty} g / f = 0$, then $f + O(g) \sim f$.
\end{theorem}

The following theorem shows that $\sim$ is a transitive relation. This property is used implicitly by proofs that chain multiple $\sim$s in the form $f \sim g \sim h$, then conclude that $f \sim h$.

\begin{theorem}
\label{thm:Transitivity}
	If $f \sim g$ and $g \sim h$, then $f \sim h$.
\end{theorem}

The following theorem states that if two functions are asymptotic, then they belong to the same big-O class.

\begin{theorem}
\label{thm:SameBigOClass}
	If $f \sim g$ and $g = O(h)$, then $f = O(h)$.
\end{theorem}