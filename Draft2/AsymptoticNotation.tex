\subsection{Motivation}

In order to highlight the benefit of growth arrays, I will use an alternative notation for time complexity. The reason for this is, for certain operations, growth arrays are only better than dynamic arrays by a constant factor. For example, dynamic arrays might allocate roughly $2n$ memory for appending $n$ items, while growth arrays would allocate roughly $n$ memory. Even though growth arrays are clearly better in this regard, the big-O space complexity for both data structures would be the same, $O(n)$.

My goal is to be able to compare the coefficients of the highest-order terms in both expressions. For example, I would like to take the ratio $\frac{2n}{n}$, see that it is $2$, and conclude that dynamic arrays allocate roughly twice as much as growth arrays for large $n$. However, big-O notation does not support this.

\subsection{Definition}

I will use the $\TextTilde$ relation to analyze time complexity. It is defined as follows:

\begin{align*}
f(n) \TextTilde g(n) \leftrightarrow \lim_{n \to \infty} \frac{g(n)}{f(n)} = 1
\end{align*}

$f$ and $g$ may be used as shorthand to denote $f(n)$ and $g(n)$, respectively.

Notice that while $O(2n) = O(n)$, $2n \TextTildeNot n$. Thus, $\TextTilde$ makes it possible to distinguish between a function that uses $n$ space and one that uses $2n$ space. \HdrNote A consequence of this is that bases for logarithms cannot be omitted, like in big-O notation.

\subsection{Properties}
\label{subsec:AsymptoticProperties}

A useful property $\TextTilde$ shares with $O$ is that it merges over arithmetic operations. This is stated by the following two theorems, which are proved in \ref{pf:MergingOverArithmetic}.

\begin{theorem}
	$P$ merges over addition and subtraction. That is, for functions $f$ and $g$,
	\begin{align*}
	\hat{f} \TextTilde f \land \hat{g} \TextTilde g \rightarrow \begin{cases}
		(\hat{f} + \hat{g}) \TextTilde (f + g)\\
		(\hat{f} - \hat{g}) \TextTilde (f - g)
	\end{cases}
	\end{align*}
\end{theorem}

\begin{theorem}
	$P$ merges over multiplication and division. That is, for functions $f$ and $g$,
	\begin{align*}
	\hat{f} \TextTilde f \land \hat{g} \TextTilde g \rightarrow \begin{cases}
	\hat{f}\hat{g} \TextTilde fg\\
	\hat{f} / \hat{g} \TextTilde f / g
	\end{cases}
	\end{align*}
\end{theorem}

Another nice property $\TextTilde$ and $O$ have in common is that lower-order terms are removed from consideration. For example, $n \TextTilde n + \log_2 n$. This is stated by the below theorem, which is proved in \ref{pf:RemovesLowerOrderTerms}.

\begin{theorem}
	If $\lim_{n \to \infty} \frac{g(n)}{f(n)} = 0$, then $f(n) + g(n) \TextTilde f(n)$.
\end{theorem}