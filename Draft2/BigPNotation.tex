\subsection{Motivation}

In order to highlight the benefit of growth arrays, I will define a new notation for time complexity. The reason for this is, for certain operations, growth arrays are only better than dynamic arrays by a constant factor. For example, dynamic arrays might allocate roughly $2n$ memory for appending $n$ items, while growth arrays would allocate roughly $n$ memory. Even though growth arrays are clearly better in this regard, the big-O space complexity for both data structures would be the same, $O(n)$.

My goal is to be able to compare the coefficients of the highest-order terms in both expressions. For example, I would like to take the ratio $\frac{2n}{n}$, see that it is $2$, and conclude that dynamic arrays allocate roughly twice as much as growth arrays for large $n$. However, big-O notation does not support this.

\subsection{Definition}

I define an alternative to big-O notation, which I will call \textbf{big-P notation}. I define $P(f(n))$ to be the class of functions $g$ such that $g(n) \to f(n)$ for large $n$. Formally,

$$
g \in P(f(n)) \leftrightarrow \lim_{n \to \infty} \frac{g(n)}{f(n)} = 1
$$

Notice that while $O(2n) = O(n)$, $P(2n) \neq P(n)$. Thus, $P$ makes it possible to distinguish between a function that uses $n$ space and one that uses $2n$ space. \textbf{Note:} A consequence of this is that bases for logarithms must be specified in big-P notation.

\subsection{Properties}
\label{BigPProperties}

\textbf{Note:} Throughout this paper, when $P(f(n))$ or $O(f(n))$ is used in an arithmetic expression, I am referring to a function in the set and not the set itself. For example, the verbal equivalent of $\frac{P(2n)}{P(n)} = P(2)$ states "the quotient of a function in $P(2n)$ and a function in $P(n)$ is equal to a function in $P(2)$."

A useful property $P$ shares with $O$ is that it merges over arithmetic operations. This is stated by the following two theorems, which are proved in \ref{MergingOverArithmetic}.

\begin{theorem}
	$P$ merges over addition and subtraction. That is,
	\begin{align*}
	P(f(n)) + P(g(n)) &= P(f(n) + g(n))\\
	P(f(n)) - P(g(n)) &= P(f(n) - g(n))
	\end{align*}
\end{theorem}

\begin{theorem}
	$P$ merges over multiplication and division. That is,
	\begin{align*}
	P(f(n))P(g(n)) &= P(f(n)g(n))\\
	\frac {P(f(n))} {P(g(n))} &= P\left(\frac{f(n)} {g(n)}\right)
	\end{align*}
\end{theorem}

Another nice property $P$ and $O$ have in common is that lower-order terms are removed from consideration. For example, $P(n) = P(n + \log_2 n)$. This is stated by the below theorem, which is proved in \ref{ScrubsLowerOrderTerms}.

\begin{theorem}
	If $\lim_{n \to \infty} \frac{g(n)}{f(n)} = 0$, then $P(f(n) + g(n)) = P(f(n))$.
\end{theorem}