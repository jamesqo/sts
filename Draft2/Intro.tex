In imperative languages, the dynamic array is the most common data structure used by programs. People often want to add multiple items to a collection and then iterate it, which dynamic arrays make simple and efficient. But can it be said they have the \textit{most} efficient algorithm for this pattern?

In this paper, I introduce an alternative data structure to the dynamic array, called the \textbf{growth array}. It is more efficient than the dynamic array at appending large numbers of items. This is due to how it `grows' once it cannot fit more items in its buffer.

When dynamic arrays run out of space, they allocate a new buffer, copy the old one's contents into it, then throw the old one away. Growth arrays are less wasteful in this scenario. Instead of discarding the filled buffer, they simply archive it. The new buffer they allocate represents a continuation of the items from the old buffer. For example, if the old buffer contained items $0 -- 9$, the new buffer would contain items $10$ and beyond. Because of this technique, growth arrays allocate less memory to store the same number of items, and do not need to copy the contents of old buffers.

Growth arrays have caveats, however. For operations other than appending, they perform no better, or slightly worse, than dynamic arrays. In particular, random access is very costly for growth arrays. They are also not contiguous in memory, which gives them poorer locality than dynamic arrays, and prevents them from being passed to external code that accepts contiguous buffers.

It is worth mentioning that if the size of the data is known in advance, both dynamic and growth arrays are redundant. One could simply allocate a raw array with the known size, and append items to it just as quickly. Thus, growth arrays are only beneficial for cases where the amount of data to be appended is unknown, but is expected to be large.