In imperative languages, the dynamic array is the most common data structure used by programs\footnote{}. People often want to add multiple items to a list and then iterate it, which dynamic arrays make simple and efficient. But can it be said dynamic arrays have the \textit{most} efficient algorithm for this pattern?

In this paper, I introduce an alternative data structure to the dynamic array, called the \textbf{growth array}. It is more efficient than the dynamic array at appending, particularly for large numbers of items. This is due to how it 'grows' once it cannot fit any more items in its buffer.

When dynamic arrays run out of space, they allocate a new buffer, copy the contents of the old one into it, and throw the old one away. However, growth arrays are less wasteful. Instead of throwing away the filled buffer, they keep it a part of the data structure. The new buffer they allocate represents a continuation of the items from the old buffer. For example, if the old buffer contained items $0-31$, the new buffer would contain items $32$ and beyond. Because of this, growth arrays allocate less memory to store the same number of items, and they do not need to copy items from the old buffer to the new one.

Growth arrays have some caveats, however. They perform no better, or slightly worse, than dynamic arrays with respect to other operations. In particular, random access involves significantly more instructions than it does for dynamic arrays. Additionally, since growth arrays are not contiguous in memory, they may have poorer locality than dynamic arrays, and cannot be passed to external code that accepts contiguous buffers.

It is also worth mentioning that if the size of the data is known in advance, both dynamic and growth arrays are completely unnecessary. A raw array could simply be allocated with the known size, and items could be appended more quickly to it. Thus, growth arrays are only advantageous for scenarios where an unknown amount of data will be appended.