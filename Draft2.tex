\documentclass{article}
\usepackage{amsmath}

\newcommand{\descriptn}{\textbf{Description}}
\newcommand{\dynarrayimpl}{\textbf{Dynamic array implementation}}
\newcommand{\funarrayimpl}{\textbf{Funnel array implementation}}
\newcommand{\tcomplex}{\textbf{Time complexity}}
\newcommand{\scomplex}{\textbf{Space complexity}}
\newcommand{\tcomplexcmp}{\textbf{Time complexity comparison}}
\newcommand{\scomplexcmp}{\textbf{Space complexity comparison}}

\newcommand{\timefn}{T}
\newcommand{\spacefn}{S}
\newcommand{\nwritesfn}{W}

\newcommand{\timenewfn}{\timefn'}
\newcommand{\spacenewfn}{\spacefn'}
\newcommand{\nwritesnewfn}{\nwritesfn'}

\newcommand{\nwritesresizefn}{W_R}

\newcommand{\timeratio}{r_\timefn}
\newcommand{\spaceratio}{r_\spacefn}

\newcommand{\bigo}{O}
\newcommand{\biggo}{P}
\newcommand{\varnitems}{n}
\newcommand{\indexertime}{I}

\newcommand{\listname}{L}

\newcommand{\initcapacity}{C_f}
\newcommand{\growthfactor}{G}
\newcommand{\capacityfn}{C}

\newcommand{\initcapacitynew}{\initcapacity'}
\newcommand{\growthfactornew}{\growthfactor'}

\setlength{\parskip}{1em}

\begin{document}
	\begin{abstract}
	\end{abstract}

	\section{Introduction}
	
	\section{Fields and Properties}
	
	I define some \textbf{fields} and \textbf{properties} (constant time methods that do not change state) for both types of arrays, so I may use them later to implement non-trivial methods.
	
	\section{Common Operations}
	
	In this section, I implement and analyze common operations for dynamic and funnel arrays. I compare both implementations' time complexities, and space complexities if the operation allocates memory.
	
	The following mathematical definitions will be used while analyzing time and space complexity:
	
	\begin{description}
		\item[$\biggo(f(n))$] This is an alternative to big-O notation that I will name "big-P notation". It is similar to big-O, but it preserves the coefficient of the fastest-growing term in $f(n)$. For example, $\bigo(2n) = \bigo(n)$, but $\biggo(2n) \neq \biggo(n)$. This makes it possible to compare two time complexities, if their ratio tends to a constant for large $n$.\\
		More formally, $\biggo(f(n)) = \biggo(g(n))$ iff $$\lim_{n \to \infty} {\frac{f(n)}{g(n)}} = 1$$.
	\end{description}
	
	\subsection{Adding}
	
	\descriptn
	
	Adding is the most common operation done on dynamic arrays\footnote{}. Funnel arrays improve the performance of adding in two ways: by allocating less memory, and reducing the amount of copying. We begin with the implementation for dynamic arrays.
	
	I will first implement and analyze adding for dynamic arrays. Let $\listname$ be a dynamic array. The following definitions are used in the code:
	
	\begin{description}
		\item[initial capacity] The capacity of a dynamic a array when one item is added to it. I denote this $\initcapacity$.\\
		(The capacity of any dynamic array with size zero should be zero.)
		\item[growth factor] The factor by which the current capacity is multiplied to get the new capacity when $\listname$ is resized. I denote this with $\growthfactor$.
	\end{description}
	
	% impl.
	
	\tcomplex
	
	Before I analyze the time complexity of [add], I consider a different method for measuring its cost. Suppose I start with an empty list and $\varnitems$ elements are added. How many times is an element stored in an array? I will term the answer to this question the \textbf{write cost} of $\varnitems$ adds, and denote it $\nwritesfn(n)$.
	
	In the above code, one write is performed after the conditional, so it is apparent that $\nwritesfn(\varnitems) \geq \varnitems$ after $n$ adds. However, [resize] also does some writing, so in order to find a precise formula for $\nwritesfn(\varnitems)$ I need to analyze when [resize] is called. Consider the following lemma:
	
	[lemma]
	Let $\listname$ by a dynamic array. As $\varnitems$ are added, the sequence of capacities $\listname$ takes on is $$S_C = 0,\ \initcapacity,\ \growthfactor\initcapacity,\ \growthfactor^2\initcapacity,\ \ldots\ \growthfactor^{\lceil \log_{\growthfactor} \varnitems - \log_{\growthfactor} \initcapacity \rceil}\initcapacity$$.
	
	[proof]
	The capacity starts out at zero and becomes $\initcapacity$ once one element is added. From then on, it can only grow by a factor of $\growthfactor$ via [resize], so if $\growthfactor^i\initcapacity$ is the current capacity then $\growthfactor^{i + 1}\initcapacity$ must be the next capacity. By induction, the rest of the sequence is $$\left\{ \growthfactor^i\initcapacity \right\}_{i = 1}^k$$ for some $k$. Since the capacity is only as big as is needs to be, namely greater than or equal to $\varnitems$, $k$ is the smallest integer for which $\growthfactor^k\initcapacity \geq \varnitems$. Using this property, we now derive $k$.
	
	\begin{align*}
	\growthfactor^k\initcapacity &\geq \varnitems\\
	\growthfactor^k &\geq \frac{\varnitems}{\initcapacity}\\
	k &\geq \log_{\growthfactor} \varnitems - \log_{\growthfactor} \initcapacity\\
	k &\geq \log_{\growthfactor} \varnitems - \log_{\growthfactor} \initcapacity > k - 1\\
	k &= \big\lceil \log_{\growthfactor} \varnitems - \log_{\growthfactor} \initcapacity \big\rceil
	\end{align*}
	
	Then the final term in the sequence is $\growthfactor^{\lceil \log_{\growthfactor} \varnitems - \log_{\growthfactor} \initcapacity \rceil}\initcapacity$, completing the proof.
	
	[make lemma]
	Now, consider that to grow by $\growthfactor$ we have to call [resize]. So $\growthfactor^k\initcapacity$ being present in $S_C$ means we once called [resize] at size $\growthfactor^{k - 1}\initcapacity$ for $k \geq 1$. Then we must have called [resize] at the sizes $$S_R = 0,\ \initcapacity,\ \growthfactor\initcapacity,\ \growthfactor^2\initcapacity,\ \ldots\ \growthfactor^{\lceil \log_{\growthfactor} \varnitems - \log_{\growthfactor} \initcapacity \rceil - 1}\initcapacity$$
	
	Each time [resize] is called, we copy $i$ items where $i$ is the current size of the dynamic array. Then the total number of items copied for $\varnitems$ adds is
	
	\begin{align*}
	0 + \initcapacity + \growthfactor\initcapacity + \ldots + \growthfactor^{\lceil \log_{\growthfactor} \varnitems - \log_{\growthfactor} \initcapacity \rceil - 1}\initcapacity &= \left( \frac{\growthfactor^{\lceil \log_{\growthfactor} \varnitems - \log_{\growthfactor} \initcapacity \rceil} - 1}{\growthfactor - 1} \right) \initcapacity
	\end{align*}
	
	Finally, adding the writes made for every item in [add], I finally find an explicit formula for $\nwritesfn(\varnitems)$.
	
	[ref1]
	$$
	\nwritesfn(\varnitems) = \varnitems + \left( \frac{\growthfactor^{\lceil \log_{\growthfactor} \varnitems - \log_{\growthfactor} \initcapacity \rceil} - 1}{\growthfactor - 1} \right) \initcapacity
	$$
	
	To make this expression easier to work with, I will write it in big-P notation to approximate it for large $\varnitems$. First I note that $\log_{\growthfactor} k \leq \lceil \log_{\growthfactor} k \rceil < \left( \log_{\growthfactor} k \right) + 1$ for any $k > 0$, so $k \leq \growthfactor^{\lceil \log_{\growthfactor} k \rceil} < \growthfactor{k}$ and $\exists r \in {[1, \growthfactor)} : rk = \growthfactor^{\lceil \log_{\growthfactor} k \rceil}$. Then
	
	\begin{align*}
	\nwritesfn(\varnitems) &= \varnitems + \left( \frac{\growthfactor^{\big\lceil \log_{\growthfactor} \left( \frac {\varnitems} {\initcapacity} \right) \big\rceil} - 1}{\growthfactor - 1} \right) \initcapacity\\
	\varnitems + \left( \frac{\frac{\varnitems}{\initcapacity} - 1}{\growthfactor - 1} \right) \initcapacity \leq \nwritesfn(\varnitems) &< \varnitems + \left( \frac{\growthfactor\frac{\varnitems}{\initcapacity} - 1}{\growthfactor - 1} \right) \initcapacity\\
	\varnitems + \left( \frac{\varnitems - \initcapacity}{\growthfactor - 1} \right) \leq \nwritesfn(\varnitems) &< \varnitems + \left( \frac{\growthfactor\varnitems - \initcapacity}{\growthfactor - 1} \right)\\
	\biggo\left( \varnitems + \left( \frac{\varnitems - \initcapacity}{\growthfactor - 1} \right) \right) \leq \biggo\left( \nwritesfn(\varnitems) \right) &< \biggo\left( \varnitems + \left( \frac{\growthfactor\varnitems - \initcapacity}{\growthfactor - 1} \right) \right)\\
	\biggo\left( \left( \frac{\growthfactor}{\growthfactor - 1} \right) \varnitems \right) \leq \biggo(\left( \nwritesfn(\varnitems) \right) &< \biggo\left( \left( \frac{2\growthfactor - 1}{\growthfactor - 1} \right) \varnitems \right)
	\end{align*}
	
	\scomplex
	
	\begin{align*}
	\spacefn(\varnitems) = \biggo(2^{\lceil \log_2 \varnitems \rceil + 1}) = \bigo(\varnitems)
	\end{align*}
	
	\funarrayimpl
	
	\tcomplex
	
	I again start off by finding the write cost when $\varnitems$ are added. I give the write cost function a slightly different name, $\nwritesnewfn(\varnitems)$, so we can compare it to $\nwritesfn(\varnitems)$ later. I also use $\growthfactornew$ and $\initcapacitynew$ instead of $\growthfactor$ and $\initcapacity$ for the growth factor and initial capacity, respectively.
	
	[That lemma] still holds, since all of the assumptions made there are also true for funnel arrays. In particular, although funnel arrays resize slightly differently than dynamic arrays, the following claim is still true:
	
	[lemma]
	The capacity of a funnel array grows by a constant factor $\growthfactornew$ after the first time it is called.
	
	[proof]
	I prove that the algorithm ensures this using induction.
	
	For the base case, consider the second time [resize] is called: the current capacity is $\initcapacitynew$, and the next (total) capacity should be $\growthfactornew\initcapacitynew$. Then a buffer of size $(\growthfactornew - 1)\initcapacitynew$ should be added, and that is what the algorithm does.
	
	For $i \geq 2$, given the algorithm behaves correctly the $i$th time it is called, I wish to show it behaves correctly the $(i + 1)$th time. From the inductive hypothesis, when the algorithm is called the $(i + 1)$th time, the current capacity is $\growthfactornew^i\initcapacitynew$ and the head capacity is $(\growthfactornew^i - \growthfactornew^{i - 1})\initcapacitynew$. The next (total) capacity should be $\growthfactornew^{i + 1}\initcapacitynew$, and the next head capacity should be $(\growthfactornew^{i + 1} - \growthfactornew^i)\initcapacitynew$, which is $\growthfactornew$ times the current head capacity. The algorithm determines the capacity of the next buffer in this manner, proving the inductive step and completing the induction.
	
	Now I take a look at [resize]. Unlike dynamic arrays, the number of writes performed does not equal $i$ if the current size is $i$. No items are being copied; the only source of writes is adding a buffer to the tail. The goal is to find the number of writes that causes.
	
	Let $\nwritesresizefn(n)$ denote the total number of writes made by [resize]. From [make lemma], I know that [resize] is called at the sizes
	
	$$
	S_R = 0,\ \initcapacitynew,\ \growthfactornew\initcapacitynew,\ \growthfactornew^2\initcapacitynew,\ \ldots\ \growthfactornew^{\lceil \log_{\growthfactornew} \varnitems - \log_{\growthfactornew} \initcapacitynew \rceil - 1}\initcapacitynew
	$$
	
	Let $k$ be the number of times the tail is added to. Since the tail is a dynamic array, the number of writes it makes is $\nwritesfn(k)$. Since we call [resize] $|S_R|$ times and it adds to the tail each time except for the first, $k = |S_R| - 1 = \lceil \log_{\growthfactornew} \varnitems - \log_{\growthfactornew} \initcapacitynew \rceil$. Then the formula for $\nwritesresizefn(n)$ is
	
	$$
	\nwritesresizefn(n) = k + \left( \frac{\growthfactor^{\lceil \log_{\growthfactor} k - \log_{\growthfactor} \initcapacity \rceil} - 1}{\growthfactor - 1} \right) \initcapacity
	$$
	
	where $\initcapacity$ $\growthfactor$ are the capacity of the \textit{tail} (not the funnel array). I finally conclude that
	
	$$
	\nwritesnewfn(n) = n + k + \left( \frac{\growthfactor^{\lceil \log_{\growthfactor} k - \log_{\growthfactor} \initcapacity \rceil} - 1}{\growthfactor - 1} \right) \initcapacity
	$$
	
	I now approximate this expression with big-P notation. First, I establish an approximation for $k$ using big-O:
	
	\begin{align*}
	k &= \lceil \log_{\growthfactornew} \varnitems - \log_{\growthfactornew} \initcapacitynew \rceil\\
	\bigo(k) &= \bigo(\log \varnitems)
	\end{align*}
	
	Then, it is shown that the $k$-term disappears when analyzing the big-P complexity of $\nwritesnewfn(\varnitems)$, leaving just $\biggo(\varnitems)$:
	
	\begin{align*}
	\biggo(\nwritesnewfn(\varnitems)) &= \biggo(\varnitems + \nwritesfn(k))\\
	&= \biggo(\varnitems) + \biggo(\nwritesfn(k))\\
	&= \biggo(\varnitems) + \bigo(\log \varnitems)\\
	&= \biggo(\varnitems)
	\end{align*}
	
	\scomplex
	
	\begin{align*}
	\spacenewfn(\varnitems) = \biggo(2^{\lceil \log_2 \varnitems \rceil}) = \bigo(\varnitems)
	\end{align*}
	
	\tcomplexcmp
	
	\scomplexcmp
	
	\begin{align*}
	\spaceratio = \frac {\biggo(2^{\lceil \log_2 \varnitems \rceil})} {\biggo(2^{\lceil \log_2 \varnitems \rceil + 1})} = \frac{1}{2}
	\end{align*}
	
	\subsection{Indexing}
	
	\descriptn
	
	Indexing is another very common operation on a list. I will call methods that get or set an item at a specified index \textbf{get indexer} and \textit{set indexers}, respectively.
	
	\dynarrayimpl
	
	\tcomplex
	
	$\timefn(\varnitems) = \bigo(1)$
	
	% <funnel array implementation>
	
	\tcomplex
	
	$\timenewfn(\varnitems) = \bigo(1)$
	
	\tcomplexcmp
	
	\subsection{Iterating}
	
	\descriptn
	
	\textbf{Iteration} of a list is the process of performing some action on each of its elements.
	
	\dynarrayimpl
	
	\tcomplex
	
	Ignoring the arbitrary code run after getting each element, the time complexity of this method is $\bigo(\varnitems)$.
	
	\funarrayimpl
	
	\tcomplex
	
	$\timenewfn(\varnitems) = \bigo(\varnitems)$
	
	\tcomplexcmp
	
	\subsection{Copying to an array}
	
	\descriptn
	
	Users often want to take list structures, such as dynamic arrays, and convert them into plain arrays. There are multiple reasons why someone would want to do this after they are done adding to the list:
	
	\begin{itemize}
		\item Plain arrays hold on to exactly the amount of memory needed to hold their elements. However, dynamic and funnel arrays allocate more space than necessary to optimize adding new items.
		\item The user wants to call a function in third-party code that takes a plain array as an argument.
		\item
		\item Plain arrays are contiguous, while funnel arrays are fragmented and have worse locality.
		\item The indexer of funnel arrays is several times slower than that of plain arrays, whether the $\bigo(1)$ or $\bigo(\log \varnitems)$ implementation is chosen.
	\end{itemize}
	
	\dynarrayimpl
	
	\tcomplex
	
	\funarrayimpl
	
	\tcomplex
	
	\section{Other Operations}
	
	\subsection{Inserting}
	
	\descriptn
	
	\textbf{Inserting} an element places it at a specified index within the list, and increments the list's count. If the index equals the size of the list, the effect is the same as adding the element. Otherwise, the elements at indices greater than or equal to the specified index are moved to the next index, then the element is placed at the specified index.
	
	\subsection{Deleting}
	
	\descriptn
	
	\textbf{Deleting} the element at a specified index moves the elements at indices greater than the specified index to the previous index. The count of the list is decremented.
	
	\subsection{Searching}
	
	\descriptn
	
	\textbf{Searching} for an element returns the first or last index within the list where the item can be found. If the user knows the items of the lists are sorted, \textbf{binary search} can be used.
	
	\subsection{In-place sorting}
	
	\descriptn
	
	Given a strict ordering $<$, we say a list $\listname$ is \textbf{sorted} by $<$ iff $\left(a, b \in \listname \land a < b\right) \leftrightarrow I_M(a) < I_m(b)$. $I_M(a)$ is the last (maximum) index of $a$ in $L$, and $I_m(b)$ is the first (minimum) index of $b$ in $L$. Note that the use of $<$ on the right-hand side compares integers and not elements of $L$, so this is not a recursive definition.
	
	\section{Implementations}
	
	\section{Benchmarks}
	
	\section{Closing Remarks}

\end{document}
